{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a92fb82-0f5c-4c2b-8b91-ac801c0ccaac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The Illustrated Transformer \n",
    "# http://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b395a66-a6f1-4fcc-8e65-cc147cde4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Input, Add, Concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cdbbaf7-41c6-41af-b15b-8e4026fb3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "num_encoder_tokens = 128\n",
    "num_decoder_tokens = 128\n",
    "max_encoder_seq_length = 512 # 512 in original paper\n",
    "max_decoder_seq_length = 512 # 512 in original paper\n",
    "latent_dim = 64 # 64 in original pape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32bf0d21-6be5-4237-9572-1ae16cf80e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.units = units\n",
    "        self.Key = Dense(units = self.units, activation = 'relu')\n",
    "        self.Query = Dense(units = self.units, activation = 'relu')\n",
    "        self.Value = Dense(units = self.units, activation = 'relu')\n",
    "    def call(self, encoder_input):\n",
    "        K = self.Key(encoder_input) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        Q = self.Query(encoder_input) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        V = self.Value(encoder_input) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        x = tf.matmul(Q,K,transpose_b = True)/8 # sqrt(latent_dim) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        x = tf.nn.softmax(x,axis = 2) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        return tf.matmul(x,V) # Shape(N, num_encoder_tokens, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "440475a2-cf3f-48ef-9966-2c75be9ca2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.layers.Layer):\n",
    "    def __init__(self, latent_dim, max_encoder_seq_length):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_encoder_seq_length = max_encoder_seq_length\n",
    "        self.selfattention_layer_1 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_2 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_3 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_4 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_5 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_6 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_7 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_8 = SelfAttention(self.latent_dim)\n",
    "        self.cat = Concatenate(axis=2)\n",
    "        self.reduce_dim = Dense(units = self.latent_dim, activation = 'relu')\n",
    "        self.up_dim = Dense(units = self.max_encoder_seq_length, activation = 'relu')\n",
    "        self.add_norm_1 = BatchNormalization()\n",
    "        self.add_norm_2 = BatchNormalization()\n",
    "        self.add= Add()\n",
    "        self.feed_forward = Dense(units = self.max_encoder_seq_length, activation = 'relu')\n",
    "    def call(self, encoder_input):\n",
    "        selfattention_layer_1_output = self.selfattention_layer_1(encoder_input)\n",
    "        selfattention_layer_2_output = self.selfattention_layer_2(encoder_input)\n",
    "        selfattention_layer_3_output = self.selfattention_layer_3(encoder_input)\n",
    "        selfattention_layer_4_output = self.selfattention_layer_4(encoder_input)\n",
    "        selfattention_layer_5_output = self.selfattention_layer_5(encoder_input)\n",
    "        selfattention_layer_6_output = self.selfattention_layer_6(encoder_input)\n",
    "        selfattention_layer_7_output = self.selfattention_layer_7(encoder_input)\n",
    "        selfattention_layer_8_output = self.selfattention_layer_8(encoder_input)\n",
    "        x = self.cat([selfattention_layer_1_output,selfattention_layer_2_output,\n",
    "                    selfattention_layer_3_output, selfattention_layer_4_output,\n",
    "                    selfattention_layer_5_output, selfattention_layer_6_output,\n",
    "                    selfattention_layer_7_output, selfattention_layer_8_output])\n",
    "        x = self.reduce_dim(x)\n",
    "        x = self.up_dim(x)\n",
    "        add_norm_output = self.add_norm_1(self.add([encoder_input,x]),training=True)\n",
    "        x = self.feed_forward(add_norm_output)\n",
    "        return self.add_norm_2(self.add([add_norm_output,x]),training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0460b1a9-32a0-4292-914c-bdeca9c93486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderAttention(keras.layers.Layer):\n",
    "    def __init__(self, units, dim):\n",
    "        super(EncoderDecoderAttention,self).__init__()\n",
    "        self.units = units\n",
    "        self.dim = dim\n",
    "        self.Key = Dense(units = self.units, activation = 'relu')\n",
    "        self.Query = Dense(units = self.units, activation = 'relu')\n",
    "        self.Value = Dense(units = self.units, activation = 'relu')\n",
    "        self.up_dim_context = Dense(units = self.dim, activation = 'relu')\n",
    "    def call(self, encoder_output, decoder_input):\n",
    "        K = self.Key(encoder_output) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        Q = self.Query(decoder_input) # Shape(N, num_decoder_tokens, latent_dim)\n",
    "        V = self.Value(encoder_output) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        x = tf.matmul(Q,K,transpose_b = True)/8 # sqrt(latent_dim) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        x = tf.nn.softmax(x,axis = 2) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        x = tf.matmul(x,V) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        return self.up_dim_context(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29533a58-8e63-4a7b-a345-88ec47c23d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self, latent_dim, max_decoder_seq_length):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_decoder_seq_length = max_decoder_seq_length\n",
    "        self.selfattention_layer_1 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_2 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_3 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_4 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_5 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_6 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_7 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_8 = SelfAttention(self.latent_dim)\n",
    "        self.cat = Concatenate(axis=2)\n",
    "        self.reduce_dim = Dense(units = self.latent_dim, activation = 'relu')\n",
    "        self.up_dim = Dense(units = self.max_decoder_seq_length, activation = 'relu')\n",
    "        self.add_norm_1 = BatchNormalization()\n",
    "        self.add_norm_2 = BatchNormalization()\n",
    "        self.add_norm_3 = BatchNormalization()\n",
    "        self.add= Add()\n",
    "        self.feed_forward = Dense(units = self.max_decoder_seq_length, activation = 'relu')\n",
    "        self.en_de_attention = EncoderDecoderAttention(self.latent_dim, self.max_decoder_seq_length)\n",
    "    def call(self, encoder_output, decoder_input):\n",
    "        selfattention_layer_1_output = self.selfattention_layer_1(decoder_input)\n",
    "        selfattention_layer_2_output = self.selfattention_layer_2(decoder_input)\n",
    "        selfattention_layer_3_output = self.selfattention_layer_3(decoder_input)\n",
    "        selfattention_layer_4_output = self.selfattention_layer_4(decoder_input)\n",
    "        selfattention_layer_5_output = self.selfattention_layer_5(decoder_input)\n",
    "        selfattention_layer_6_output = self.selfattention_layer_6(decoder_input)\n",
    "        selfattention_layer_7_output = self.selfattention_layer_7(decoder_input)\n",
    "        selfattention_layer_8_output = self.selfattention_layer_8(decoder_input)\n",
    "        x = self.cat([selfattention_layer_1_output,selfattention_layer_2_output,\n",
    "                    selfattention_layer_3_output, selfattention_layer_4_output,\n",
    "                    selfattention_layer_5_output, selfattention_layer_6_output,\n",
    "                    selfattention_layer_7_output, selfattention_layer_8_output])\n",
    "        x = self.reduce_dim(x)\n",
    "        x = self.up_dim(x)\n",
    "        add_norm_output_1 = self.add_norm_1(self.add([decoder_input,x]),training=True)\n",
    "        x = self.en_de_attention(encoder_output, add_norm_output_1)\n",
    "        add_norm_output_2 = self.add_norm_2(self.add([add_norm_output_1,x]),training=True)\n",
    "        x = self.feed_forward(add_norm_output_2)\n",
    "        return self.add_norm_3(self.add([add_norm_output_2,x]),training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a14016-5277-42b1-b326-9fb4a4ba718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_input = Input(shape=(None, max_encoder_seq_length))\n",
    "encoder_block_1 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "x = encoder_block_1(encoder_input)\n",
    "encoder_block_2 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "x = encoder_block_2(x)\n",
    "encoder_block_3 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "x = encoder_block_3(x)\n",
    "encoder_block_4 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "x = encoder_block_4(x)\n",
    "encoder_block_5 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "x = encoder_block_5(x)\n",
    "encoder_block_6 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "encoder_block_6_output = encoder_block_6(x)\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(None, max_decoder_seq_length))\n",
    "decoder_block_1 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_1(encoder_block_6_output, decoder_input)\n",
    "decoder_block_2 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_2(encoder_block_6_output, x)\n",
    "decoder_block_3 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_3(encoder_block_6_output, x)\n",
    "decoder_block_4 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_4(encoder_block_6_output, x)\n",
    "decoder_block_5 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_5(encoder_block_6_output, x)\n",
    "decoder_block_6 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_6(encoder_block_6_output, x)\n",
    "# Linear - Softmax\n",
    "linear = Dense(units = latent_dim, activation = 'relu')\n",
    "x = linear(x)\n",
    "softmax = Dense(units = max_decoder_seq_length, activation = 'softmax')\n",
    "output= softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ceb07fc-8ba5-4f88-9557-ebe6cde8bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_input,decoder_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce81cf25-2327-4269-9754-5baee7250f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66c63313-e47c-452a-b907-a372ddf246e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, None, 512)]  0           []                               \n",
      "                                                                                                  \n",
      " encoder_3 (Encoder)            (None, None, 512)    1120832     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_4 (Encoder)            (None, None, 512)    1120832     ['encoder_3[0][0]']              \n",
      "                                                                                                  \n",
      " encoder_5 (Encoder)            (None, None, 512)    1120832     ['encoder_4[0][0]']              \n",
      "                                                                                                  \n",
      " encoder_6 (Encoder)            (None, None, 512)    1120832     ['encoder_5[0][0]']              \n",
      "                                                                                                  \n",
      " encoder_7 (Encoder)            (None, None, 512)    1120832     ['encoder_6[0][0]']              \n",
      "                                                                                                  \n",
      " encoder_8 (Encoder)            (None, None, 512)    1120832     ['encoder_7[0][0]']              \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, None, 512)]  0           []                               \n",
      "                                                                                                  \n",
      " decoder_2 (Decoder)            (None, None, 512)    1254656     ['encoder_8[0][0]',              \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      " decoder_3 (Decoder)            (None, None, 512)    1254656     ['encoder_8[0][0]',              \n",
      "                                                                  'decoder_2[0][0]']              \n",
      "                                                                                                  \n",
      " decoder_4 (Decoder)            (None, None, 512)    1254656     ['encoder_8[0][0]',              \n",
      "                                                                  'decoder_3[0][0]']              \n",
      "                                                                                                  \n",
      " decoder_5 (Decoder)            (None, None, 512)    1254656     ['encoder_8[0][0]',              \n",
      "                                                                  'decoder_4[0][0]']              \n",
      "                                                                                                  \n",
      " decoder_6 (Decoder)            (None, None, 512)    1254656     ['encoder_8[0][0]',              \n",
      "                                                                  'decoder_5[0][0]']              \n",
      "                                                                                                  \n",
      " decoder_7 (Decoder)            (None, None, 512)    1254656     ['encoder_8[0][0]',              \n",
      "                                                                  'decoder_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_421 (Dense)              (None, None, 64)     32832       ['decoder_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_422 (Dense)              (None, None, 512)    33280       ['dense_421[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,319,040\n",
      "Trainable params: 14,288,320\n",
      "Non-trainable params: 30,720\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "368bd31d-f73e-4190-a458-219617a5f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# input_encoder = np.random.randint(10,size = (N, num_encoder_tokens, max_encoder_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60f93621-724d-4304-bf0d-17c169e36270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# input_decoder = np.random.randint(10,size = (N, num_decoder_tokens, max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c1afe32-1e66-4e94-b8d8-9724734021d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_decoder = model([input_encoder,input_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb4f7dc3-75b0-4a36-b6ca-5b050185dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit([input_encoder, input_decoder],output_decoder,batch_size = 50, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f461ef6-6ac6-4ef3-bad1-e108d6c0bc12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
