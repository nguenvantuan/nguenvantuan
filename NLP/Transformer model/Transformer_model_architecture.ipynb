{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a92fb82-0f5c-4c2b-8b91-ac801c0ccaac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The Illustrated Transformer \n",
    "# http://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b395a66-a6f1-4fcc-8e65-cc147cde4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Input, Add, Concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cdbbaf7-41c6-41af-b15b-8e4026fb3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "num_encoder_tokens = 20\n",
    "num_decoder_tokens = 30\n",
    "max_encoder_seq_length = 128 # 512 in original paper\n",
    "max_decoder_seq_length = 128 # 512 in original paper\n",
    "latent_dim = 32 # 64 in original pape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32bf0d21-6be5-4237-9572-1ae16cf80e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.units = units\n",
    "        self.Key = Dense(units = self.units, activation = 'relu')\n",
    "        self.Query = Dense(units = self.units, activation = 'relu')\n",
    "        self.Value = Dense(units = self.units, activation = 'relu')\n",
    "    def call(self, encoder_input):\n",
    "        K = self.Key(encoder_input) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        Q = self.Query(encoder_input) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        V = self.Value(encoder_input) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        x = tf.matmul(Q,K,transpose_b = True)/8 # sqrt(latent_dim) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        x = tf.nn.softmax(x,axis = 2) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        return tf.matmul(x,V) # Shape(N, num_encoder_tokens, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "440475a2-cf3f-48ef-9966-2c75be9ca2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.layers.Layer):\n",
    "    def __init__(self, latent_dim, max_encoder_seq_length):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_encoder_seq_length = max_encoder_seq_length\n",
    "        self.selfattention_layer_1 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_2 = SelfAttention(self.latent_dim)\n",
    "        self.cat = Concatenate(axis=2)\n",
    "        self.reduce_dim = Dense(units = self.latent_dim, activation = 'relu')\n",
    "        self.up_dim = Dense(units = self.max_encoder_seq_length, activation = 'relu')\n",
    "        self.add_norm_1 = BatchNormalization()\n",
    "        self.add_norm_2 = BatchNormalization()\n",
    "        self.add= Add()\n",
    "        self.feed_forward = Dense(units = self.max_encoder_seq_length, activation = 'relu')\n",
    "    def call(self, encoder_input):\n",
    "        selfattention_layer_1_output = self.selfattention_layer_1(encoder_input)\n",
    "        selfattention_layer_2_output = self.selfattention_layer_2(encoder_input)\n",
    "        x = self.cat([selfattention_layer_1_output,selfattention_layer_2_output])\n",
    "        x = self.reduce_dim(x)\n",
    "        x = self.up_dim(x)\n",
    "        add_norm_output = self.add_norm_1(self.add([encoder_input,x]),training=True)\n",
    "        x = self.feed_forward(add_norm_output)\n",
    "        return self.add_norm_2(self.add([add_norm_output,x]),training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0460b1a9-32a0-4292-914c-bdeca9c93486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderAttention(keras.layers.Layer):\n",
    "    def __init__(self, units, dim):\n",
    "        super(EncoderDecoderAttention,self).__init__()\n",
    "        self.units = units\n",
    "        self.dim = dim\n",
    "        self.Key = Dense(units = self.units, activation = 'relu')\n",
    "        self.Query = Dense(units = self.units, activation = 'relu')\n",
    "        self.Value = Dense(units = self.units, activation = 'relu')\n",
    "        self.up_dim_context = Dense(units = self.dim, activation = 'relu')\n",
    "    def call(self, encoder_output, decoder_input):\n",
    "        K = self.Key(encoder_output) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        Q = self.Query(decoder_input) # Shape(N, num_decoder_tokens, latent_dim)\n",
    "        V = self.Value(encoder_output) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        x = tf.matmul(Q,K,transpose_b = True)/8 # sqrt(latent_dim) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        x = tf.nn.softmax(x,axis = 2) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        x = tf.matmul(x,V) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        return self.up_dim_context(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29533a58-8e63-4a7b-a345-88ec47c23d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self, latent_dim, max_decoder_seq_length):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_decoder_seq_length = max_decoder_seq_length\n",
    "        self.selfattention_layer_1 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_2 = SelfAttention(self.latent_dim)\n",
    "        self.cat = Concatenate(axis=2)\n",
    "        self.reduce_dim = Dense(units = self.latent_dim, activation = 'relu')\n",
    "        self.up_dim = Dense(units = self.max_decoder_seq_length, activation = 'relu')\n",
    "        self.add_norm_1 = BatchNormalization()\n",
    "        self.add_norm_2 = BatchNormalization()\n",
    "        self.add_norm_3 = BatchNormalization()\n",
    "        self.add= Add()\n",
    "        self.feed_forward = Dense(units = self.max_decoder_seq_length, activation = 'relu')\n",
    "        self.en_de_attention = EncoderDecoderAttention(self.latent_dim, self.max_decoder_seq_length)\n",
    "    def call(self, encoder_output, decoder_input):\n",
    "        selfattention_layer_1_output = self.selfattention_layer_1(decoder_input)\n",
    "        selfattention_layer_2_output = self.selfattention_layer_2(decoder_input)\n",
    "        x = self.cat([selfattention_layer_1_output,selfattention_layer_2_output])\n",
    "        x = self.reduce_dim(x)\n",
    "        x = self.up_dim(x)\n",
    "        add_norm_output_1 = self.add_norm_1(self.add([decoder_input,x]),training=True)\n",
    "        x = self.en_de_attention(encoder_output, add_norm_output_1)\n",
    "        add_norm_output_2 = self.add_norm_2(self.add([add_norm_output_1,x]),training=True)\n",
    "        x = self.feed_forward(add_norm_output_2)\n",
    "        return self.add_norm_3(self.add([add_norm_output_2,x]),training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21a14016-5277-42b1-b326-9fb4a4ba718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_input = Input(shape=(None, max_encoder_seq_length))\n",
    "encoder_block_1 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "x = encoder_block_1(encoder_input)\n",
    "encoder_block_2 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "encoder_block_2_output = encoder_block_2(x)\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(None, max_decoder_seq_length))\n",
    "decoder_block_1 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_1(encoder_block_2_output, decoder_input)\n",
    "decoder_block_2 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_2(encoder_block_2_output, x)\n",
    "# Linear - Softmax\n",
    "linear = Dense(units = latent_dim, activation = 'relu')\n",
    "x = linear(x)\n",
    "softmax = Dense(units = max_decoder_seq_length, activation = 'softmax')\n",
    "output= softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ceb07fc-8ba5-4f88-9557-ebe6cde8bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_input,decoder_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce81cf25-2327-4269-9754-5baee7250f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66c63313-e47c-452a-b907-a372ddf246e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " encoder_6 (Encoder)            (None, None, 128)    48608       ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_7 (Encoder)            (None, None, 128)    48608       ['encoder_6[0][0]']              \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " decoder_6 (Decoder)            (None, None, 128)    65728       ['encoder_7[0][0]',              \n",
      "                                                                  'input_8[0][0]']                \n",
      "                                                                                                  \n",
      " decoder_7 (Decoder)            (None, None, 128)    65728       ['encoder_7[0][0]',              \n",
      "                                                                  'decoder_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_182 (Dense)              (None, None, 32)     4128        ['decoder_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_183 (Dense)              (None, None, 128)    4224        ['dense_182[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 237,024\n",
      "Trainable params: 234,464\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "368bd31d-f73e-4190-a458-219617a5f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "input_encoder = np.random.randint(10,size = (N, num_encoder_tokens, max_encoder_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60f93621-724d-4304-bf0d-17c169e36270",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "input_decoder = np.random.randint(10,size = (N, num_decoder_tokens, max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c1afe32-1e66-4e94-b8d8-9724734021d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_decoder = model([input_encoder,input_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb4f7dc3-75b0-4a36-b6ca-5b050185dcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 4s 27ms/step - loss: 4.9450 - accuracy: 0.0240\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.8738 - accuracy: 0.0557\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.8416 - accuracy: 0.0840\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.8247 - accuracy: 0.1093\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.8139 - accuracy: 0.1210\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.8061 - accuracy: 0.1367\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.8001 - accuracy: 0.1550\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7952 - accuracy: 0.1710\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7909 - accuracy: 0.1903\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7873 - accuracy: 0.2047\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.7837 - accuracy: 0.2257\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7805 - accuracy: 0.2357\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7775 - accuracy: 0.2380\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 4.7745 - accuracy: 0.2503\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7717 - accuracy: 0.2697\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7689 - accuracy: 0.2750\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7664 - accuracy: 0.2857\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 4.7638 - accuracy: 0.2933\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7612 - accuracy: 0.3067\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7588 - accuracy: 0.3183\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 4.7568 - accuracy: 0.3203\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7544 - accuracy: 0.3320\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.7524 - accuracy: 0.3480\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7503 - accuracy: 0.3483\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.7486 - accuracy: 0.3577\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7466 - accuracy: 0.3733\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 4.7447 - accuracy: 0.3687\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 4.7430 - accuracy: 0.3810\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7411 - accuracy: 0.3900\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.7396 - accuracy: 0.4010\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7381 - accuracy: 0.4133\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7366 - accuracy: 0.4160\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7352 - accuracy: 0.4157\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7340 - accuracy: 0.4360\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 4.7327 - accuracy: 0.4347\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7313 - accuracy: 0.4440\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7304 - accuracy: 0.4433\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7290 - accuracy: 0.4563\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.7281 - accuracy: 0.4583\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7271 - accuracy: 0.4640\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7263 - accuracy: 0.4740\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 4.7256 - accuracy: 0.4727\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.7246 - accuracy: 0.4890\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 4.7238 - accuracy: 0.4957\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 4.7229 - accuracy: 0.5013\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7219 - accuracy: 0.5080\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7213 - accuracy: 0.5033\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.7206 - accuracy: 0.5153\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.7202 - accuracy: 0.5130\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7194 - accuracy: 0.5247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c83c18c550>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([input_encoder, input_decoder],output_decoder,batch_size = 10, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f461ef6-6ac6-4ef3-bad1-e108d6c0bc12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
