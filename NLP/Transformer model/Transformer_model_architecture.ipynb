{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303f39ab-3ee2-4453-8a8f-6ec01c04245c",
   "metadata": {},
   "source": [
    "### The Illustrated Transformer \n",
    "http://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b395a66-a6f1-4fcc-8e65-cc147cde4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Input, Add, Concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cdbbaf7-41c6-41af-b15b-8e4026fb3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "num_encoder_tokens = 20\n",
    "num_decoder_tokens = 30\n",
    "max_encoder_seq_length = 128 # 512 in original paper\n",
    "max_decoder_seq_length = 128 # 512 in original paper\n",
    "latent_dim = 32 # 64 in original pape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32bf0d21-6be5-4237-9572-1ae16cf80e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.units = units\n",
    "        self.Key = Dense(units = self.units, activation = 'relu')\n",
    "        self.Query = Dense(units = self.units, activation = 'relu')\n",
    "        self.Value = Dense(units = self.units, activation = 'relu')\n",
    "    def call(self, encoder_input):\n",
    "        K = self.Key(encoder_input) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        Q = self.Query(encoder_input) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        V = self.Value(encoder_input) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        x = tf.matmul(Q,K,transpose_b = True)/8 # sqrt(latent_dim) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        x = tf.nn.softmax(x,axis = 2) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        return tf.matmul(x,V) # Shape(N, num_encoder_tokens, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "440475a2-cf3f-48ef-9966-2c75be9ca2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.layers.Layer):\n",
    "    def __init__(self, latent_dim, max_encoder_seq_length):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_encoder_seq_length = max_encoder_seq_length\n",
    "        self.selfattention_layer_1 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_2 = SelfAttention(self.latent_dim)\n",
    "        self.cat = Concatenate(axis=2)\n",
    "        self.reduce_dim = Dense(units = self.latent_dim, activation = 'relu')\n",
    "        self.up_dim = Dense(units = self.max_encoder_seq_length, activation = 'relu')\n",
    "        self.add_norm_1 = BatchNormalization()\n",
    "        self.add_norm_2 = BatchNormalization()\n",
    "        self.add= Add()\n",
    "        self.feed_forward = Dense(units = self.max_encoder_seq_length, activation = 'relu')\n",
    "    def call(self, encoder_input):\n",
    "        selfattention_layer_1_output = self.selfattention_layer_1(encoder_input)\n",
    "        selfattention_layer_2_output = self.selfattention_layer_2(encoder_input)\n",
    "        x = self.cat([selfattention_layer_1_output,selfattention_layer_2_output])\n",
    "        x = self.reduce_dim(x)\n",
    "        x = self.up_dim(x)\n",
    "        add_norm_output = self.add_norm_1(self.add([encoder_input,x]),training=True)\n",
    "        x = self.feed_forward(add_norm_output)\n",
    "        return self.add_norm_2(self.add([add_norm_output,x]),training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0460b1a9-32a0-4292-914c-bdeca9c93486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderAttention(keras.layers.Layer):\n",
    "    def __init__(self, units, dim):\n",
    "        super(EncoderDecoderAttention,self).__init__()\n",
    "        self.units = units\n",
    "        self.dim = dim\n",
    "        self.Key = Dense(units = self.units, activation = 'relu')\n",
    "        self.Query = Dense(units = self.units, activation = 'relu')\n",
    "        self.Value = Dense(units = self.units, activation = 'relu')\n",
    "        self.up_dim_context = Dense(units = self.dim, activation = 'relu')\n",
    "    def call(self, encoder_output, decoder_input):\n",
    "        K = self.Key(encoder_output) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        Q = self.Query(decoder_input) # Shape(N, num_decoder_tokens, latent_dim)\n",
    "        V = self.Value(encoder_output) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        x = tf.matmul(Q,K,transpose_b = True)/8 # sqrt(latent_dim) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        x = tf.nn.softmax(x,axis = 2) # Shape(N, num_encoder_tokens, num_encoder_tokens)\n",
    "        x = tf.matmul(x,V) # Shape(N, num_encoder_tokens, latent_dim)\n",
    "        return self.up_dim_context(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29533a58-8e63-4a7b-a345-88ec47c23d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self, latent_dim, max_decoder_seq_length):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_decoder_seq_length = max_decoder_seq_length\n",
    "        self.selfattention_layer_1 = SelfAttention(self.latent_dim)\n",
    "        self.selfattention_layer_2 = SelfAttention(self.latent_dim)\n",
    "        self.cat = Concatenate(axis=2)\n",
    "        self.reduce_dim = Dense(units = self.latent_dim, activation = 'relu')\n",
    "        self.up_dim = Dense(units = self.max_decoder_seq_length, activation = 'relu')\n",
    "        self.add_norm_1 = BatchNormalization()\n",
    "        self.add_norm_2 = BatchNormalization()\n",
    "        self.add_norm_3 = BatchNormalization()\n",
    "        self.add= Add()\n",
    "        self.feed_forward = Dense(units = self.max_decoder_seq_length, activation = 'relu')\n",
    "        self.en_de_attention = EncoderDecoderAttention(self.latent_dim, self.max_decoder_seq_length)\n",
    "    def call(self, encoder_output, decoder_input):\n",
    "        selfattention_layer_1_output = self.selfattention_layer_1(decoder_input)\n",
    "        selfattention_layer_2_output = self.selfattention_layer_2(decoder_input)\n",
    "        x = self.cat([selfattention_layer_1_output,selfattention_layer_2_output])\n",
    "        x = self.reduce_dim(x)\n",
    "        x = self.up_dim(x)\n",
    "        add_norm_output_1 = self.add_norm_1(self.add([decoder_input,x]),training=True)\n",
    "        x = self.en_de_attention(encoder_output, add_norm_output_1)\n",
    "        add_norm_output_2 = self.add_norm_2(self.add([add_norm_output_1,x]),training=True)\n",
    "        x = self.feed_forward(add_norm_output_2)\n",
    "        return self.add_norm_3(self.add([add_norm_output_2,x]),training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a14016-5277-42b1-b326-9fb4a4ba718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_input = Input(shape=(None, max_encoder_seq_length))\n",
    "encoder_block_1 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "x = encoder_block_1(encoder_input)\n",
    "encoder_block_2 = Encoder(latent_dim, max_encoder_seq_length)\n",
    "encoder_block_2_output = encoder_block_2(x)\n",
    "# Decoder\n",
    "decoder_input = Input(shape=(None, max_decoder_seq_length))\n",
    "decoder_block_1 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_1(encoder_block_2_output, decoder_input)\n",
    "decoder_block_2 = Decoder(latent_dim, max_decoder_seq_length)\n",
    "x = decoder_block_2(encoder_block_2_output, x)\n",
    "# Linear - Softmax\n",
    "linear = Dense(units = latent_dim, activation = 'relu')\n",
    "x = linear(x)\n",
    "softmax = Dense(units = max_decoder_seq_length, activation = 'softmax')\n",
    "output= softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ceb07fc-8ba5-4f88-9557-ebe6cde8bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_input,decoder_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce81cf25-2327-4269-9754-5baee7250f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c63313-e47c-452a-b907-a372ddf246e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " encoder (Encoder)              (None, None, 128)    48608       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_1 (Encoder)            (None, None, 128)    48608       ['encoder[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None, 128)]  0           []                               \n",
      "                                                                                                  \n",
      " decoder (Decoder)              (None, None, 128)    65728       ['encoder_1[0][0]',              \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " decoder_1 (Decoder)            (None, None, 128)    65728       ['encoder_1[0][0]',              \n",
      "                                                                  'decoder[0][0]']                \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, None, 32)     4128        ['decoder_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, None, 128)    4224        ['dense_44[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 237,024\n",
      "Trainable params: 234,464\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368bd31d-f73e-4190-a458-219617a5f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "input_encoder = np.random.randint(10,size = (N, num_encoder_tokens, max_encoder_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60f93621-724d-4304-bf0d-17c169e36270",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "input_decoder = np.random.randint(10,size = (N, num_decoder_tokens, max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c1afe32-1e66-4e94-b8d8-9724734021d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_decoder = model([input_encoder,input_decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4f7dc3-75b0-4a36-b6ca-5b050185dcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 4s 30ms/step - loss: 4.7126 - accuracy: 0.8073\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 4.7134 - accuracy: 0.7697\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7128 - accuracy: 0.7923\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7123 - accuracy: 0.8017\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7121 - accuracy: 0.8127\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7120 - accuracy: 0.8193\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7121 - accuracy: 0.8287\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7119 - accuracy: 0.8243\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.7118 - accuracy: 0.8277\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.7120 - accuracy: 0.8197\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7118 - accuracy: 0.8327\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7117 - accuracy: 0.8267\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7118 - accuracy: 0.8210\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7117 - accuracy: 0.8223\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7117 - accuracy: 0.8307\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7118 - accuracy: 0.8267\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7115 - accuracy: 0.8350\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7118 - accuracy: 0.8247\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7117 - accuracy: 0.8357\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7115 - accuracy: 0.8327\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7117 - accuracy: 0.8257\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7116 - accuracy: 0.8277\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7117 - accuracy: 0.8393\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7115 - accuracy: 0.8387\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7117 - accuracy: 0.8297\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7114 - accuracy: 0.8390\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7116 - accuracy: 0.8300\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7116 - accuracy: 0.8440\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7117 - accuracy: 0.8433\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7116 - accuracy: 0.8380\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7116 - accuracy: 0.8247\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7115 - accuracy: 0.8323\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 4.7116 - accuracy: 0.8233\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7116 - accuracy: 0.8350\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 4.7115 - accuracy: 0.8370\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7116 - accuracy: 0.8353\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.7115 - accuracy: 0.8417\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7116 - accuracy: 0.8340\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7115 - accuracy: 0.8467\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7116 - accuracy: 0.8300\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 4.7114 - accuracy: 0.8473\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7114 - accuracy: 0.8463\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7113 - accuracy: 0.8407\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 4.7115 - accuracy: 0.8403\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 4.7117 - accuracy: 0.8310\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 4.7114 - accuracy: 0.8400\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 4.7115 - accuracy: 0.8437\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 4.7114 - accuracy: 0.8457\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 4.7112 - accuracy: 0.8543\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 4.7114 - accuracy: 0.8420\n",
      "<keras.callbacks.History object at 0x000002C826CE2FA0>\n"
     ]
    }
   ],
   "source": [
    "print(model.fit([input_encoder, input_decoder],output_decoder,batch_size = 10, epochs = 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab989c-2a7a-4d7a-b604-41e4b32ead3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
